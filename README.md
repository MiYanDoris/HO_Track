Notice:
1. DexYCB dataloader: replace mano with our_mano



# Jointly pose tracking for hand and category-level objects

## Introduction

This is the official PyTorch implementation of our paper. This repository is still under construction.


## Citation

If you find our work useful in your research, please consider citing:

```
@article{weng2021captra,
	title={CAPTRA: CAtegory-level Pose Tracking for Rigid and Articulated Objects from Point Clouds},
	author={Weng, Yijia and Wang, He and Zhou, Qiang and Qin, Yuzhe and Duan, Yueqi and Fan, Qingnan and Chen, Baoquan and Su, Hao and Guibas, Leonidas J},
	journal={arXiv preprint arXiv:2104.03437},
	year={2021}
```

## Installation

+ Our code has been tested with
  + Ubuntu 18.04, and macOS(CPU only)
  + CUDA 11.0
  + Python 3.7.7
  + PyTorch 1.6.0

+ We recommend using [Anaconda](https://www.anaconda.com/) to create an environment named `captra` dedicated to this repository, by running the following:

  ```bash
  conda env create -n captra python=3.7
  conda activate captra
  ```


+ Install dependencies.

  ```bash
  pip install -r requirements.txt
  ```

+ Compile the CUDA code for PointNet++ backbone.

  ```bash
  cd network/models/pointnet_lib
  python setup.py install
  ```

## Datasets
+ Follow instructions to install [the melodic version of ROS](http://wiki.ros.org/melodic/Installation) + [graspit interface](https://github.com/graspit-simulator/graspit_interface) + [mano_grasp](https://github.com/lwohlhart/mano_grasp)
  + **Note** that **graspit** only support **melodic** version of ROS on ubuntu **18.04**

+ NOCS dataset: [Download from the original NOCS dataset](https://github.com/hughw19/NOCS_CVPR2019#datasets): 

  ```
  python cp_nocs_objs.py -c bottle      #first copy the obj files for prepare_objects. It will also resize the objects to be like in real.
  cd mano_grasp/mano_grasp
  python prepare_objects.py --models_folder /home/hewang/Desktop/data/jiayi/h2o_data/objs/bottle --file_out NOCS_bottle.txt  --scales 1000
  roslaunch graspit_interface graspit_interface.launch   #in another cmd
  python generate_grasps.py --models_file NOCS_bottle.txt --path_out /home/hewang/Desktop/data/jiayi/h2o_data/grasps/bottle -n 30 -g 10
  
  cd preproc_grasps_data
  python remove_duplicate_grasp.py -c bottle
  python save_hand_mesh -c bottle
  python my_render.py -c bottle
  ```
  
+ Sapien dataset: [Download from object URDF models](http://download.cs.stanford.edu/orion/captra/sapien_urdf.tar)

  ```
  cd partnet_articulated_obj
  python urdf2obj.py -c laptop
  cd mano_grasp/mano_grasp
  python prepare_objects.py --models_folder /home/hewang/Desktop/data/jiayi/h2o_data/objs/laptop --file_out sapien_laptop.txt  --scales 1000
  roslaunch graspit_interface graspit_interface.launch    #in another cmd
  python generate_grasps.py --models_file sapien_laptop.txt --path_out /home/hewang/Desktop/data/jiayi/h2o_data/grasps/laptop -n 30 -g 20 --valid
  
  cd preproc_grasps_data
  python remove_duplicate_grasp.py -c laptop
  python save_hand_mesh -c laptop
  cd partnet_articulated_obj
  python sapien_render.py -c laptop
  ```


## Running

+ To train a hand network
  ```bash
    python network/train.py --config $config_path 
  ```

+ To test a hand network in video
  ```bash
    python network/test.py --config $config_path --track hand
  ```

+ useful parse_args: 
  + --debug: use model.visualize() and show plt figures
  + --debug_save: use model.visualize() and save plt figures to network/debug/..
  + --save:  save tracking results for generating .gif file
  
## File Structure

### Dataset Folder Structure

```bash
h2o_data
├── grasps		# annotation generated by graspit	
├── hands		# hand mesh generated from annotation in grasps
├── objs		# cp from original NOCS data and sapien data. No material.
├── render		 # main folder for network
│   ├── preproc		
│   ├── img		
│   └── splits	  		  			
├── nocs_obj_models	#original NOCS dataset
'''


## Acknowledgements

This implementation is based on the following repositories. We thank the authors for open sourcing their great works! 

+ PointNet++: [sshaoshuai/Pointnet2.PyTorch](https://github.com/sshaoshuai/Pointnet2.PyTorch) and [yanx27/Pointnet_Pointnet2_pytorch](https://github.com/yanx27/Pointnet_Pointnet2_pytorch)

+ [Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation: hughw19/NOCS_CVPR2019](https://github.com/hughw19/NOCS_CVPR2019)

+ [Category-Level Articulated Object Pose Estimation: dragonlong/articulated-pose](https://github.com/dragonlong/articulated-pose)


